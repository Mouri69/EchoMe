{"ast":null,"code":"// AI Service for integrating with free AI models\n// Supports Ollama, Hugging Face, and other free AI providers\n\nexport class AIService {\n  constructor() {\n    this.provider = 'ollama'; // 'ollama', 'huggingface', 'local'\n    this.model = 'llama2'; // 'llama2', 'mistral', 'codellama'\n    this.baseUrl = 'http://localhost:11434'; // Ollama default URL\n    this.conversationHistory = [];\n    this.maxHistory = 10; // Keep last 10 messages for context\n  }\n\n  // Set AI provider and model\n  setProvider(provider, model = null) {\n    this.provider = provider;\n    if (model) this.model = model;\n\n    // Set appropriate base URL\n    switch (provider) {\n      case 'ollama':\n        this.baseUrl = 'http://localhost:11434';\n        break;\n      case 'huggingface':\n        this.baseUrl = 'https://api-inference.huggingface.co/models';\n        break;\n      default:\n        this.baseUrl = 'http://localhost:11434';\n    }\n  }\n\n  // Add message to conversation history\n  addToHistory(role, content) {\n    this.conversationHistory.push({\n      role,\n      content\n    });\n\n    // Keep only recent messages\n    if (this.conversationHistory.length > this.maxHistory * 2) {\n      this.conversationHistory = this.conversationHistory.slice(-this.maxHistory * 2);\n    }\n  }\n\n  // Generate AI response using Ollama\n  async generateOllamaResponse(userMessage, personality) {\n    try {\n      const prompt = this.buildPrompt(userMessage, personality);\n      const response = await fetch(`${this.baseUrl}/api/generate`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          model: this.model,\n          prompt: prompt,\n          stream: false,\n          options: {\n            temperature: 0.7,\n            top_p: 0.9,\n            max_tokens: 200\n          }\n        })\n      });\n      if (!response.ok) {\n        throw new Error(`Ollama API error: ${response.status}`);\n      }\n      const data = await response.json();\n      return data.response || 'Sorry, I couldn\\'t generate a response.';\n    } catch (error) {\n      console.error('Ollama API error:', error);\n      return this.getFallbackResponse(personality, userMessage);\n    }\n  }\n\n  // Generate AI response using Hugging Face\n  async generateHuggingFaceResponse(userMessage, personality) {\n    try {\n      var _data$;\n      const prompt = this.buildPrompt(userMessage, personality);\n      const response = await fetch(`${this.baseUrl}/${this.model}`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${process.env.REACT_APP_HUGGINGFACE_TOKEN || ''}`\n        },\n        body: JSON.stringify({\n          inputs: prompt,\n          parameters: {\n            max_new_tokens: 200,\n            temperature: 0.7,\n            top_p: 0.9\n          }\n        })\n      });\n      if (!response.ok) {\n        throw new Error(`Hugging Face API error: ${response.status}`);\n      }\n      const data = await response.json();\n      return ((_data$ = data[0]) === null || _data$ === void 0 ? void 0 : _data$.generated_text) || 'Sorry, I couldn\\'t generate a response.';\n    } catch (error) {\n      console.error('Hugging Face API error:', error);\n      return this.getFallbackResponse(personality, userMessage);\n    }\n  }\n\n  // Build context-aware prompt\n  buildPrompt(userMessage, personality) {\n    const {\n      topWords,\n      slangWords,\n      formalityScore,\n      messageCount\n    } = personality;\n\n    // Create personality context\n    let personalityContext = '';\n    if (messageCount > 5) {\n      personalityContext = `You are an AI that has learned to speak like the user. `;\n      if (topWords.length > 0) {\n        personalityContext += `The user frequently uses words like: ${topWords.slice(0, 3).map(w => w.word).join(', ')}. `;\n      }\n      if (slangWords.length > 0) {\n        personalityContext += `The user uses slang like: ${Array.from(slangWords).slice(0, 3).join(', ')}. `;\n      }\n      if (formalityScore < 40) {\n        personalityContext += `The user speaks very casually and informally. `;\n      } else if (formalityScore > 70) {\n        personalityContext += `The user speaks formally and professionally. `;\n      }\n      personalityContext += `Mirror their speaking style, vocabulary, and tone. `;\n    }\n\n    // Build conversation history\n    const history = this.conversationHistory.map(msg => `${msg.role === 'user' ? 'User' : 'Assistant'}: ${msg.content}`).join('\\n');\n\n    // Create the full prompt\n    const prompt = `${personalityContext}\n\nConversation History:\n${history}\n\nUser: ${userMessage}\nAssistant:`;\n    return prompt;\n  }\n\n  // Fallback response when AI is unavailable\n  getFallbackResponse(personality, userMessage) {\n    // Use the existing personality engine as fallback\n    const {\n      PersonalityEngine\n    } = require('./personalityEngine');\n    const engine = new PersonalityEngine();\n    return engine.generateResponse(personality, userMessage);\n  }\n\n  // Main method to generate AI response\n  async generateResponse(userMessage, personality) {\n    // Add user message to history\n    this.addToHistory('user', userMessage);\n    let response = '';\n    try {\n      switch (this.provider) {\n        case 'ollama':\n          response = await this.generateOllamaResponse(userMessage, personality);\n          break;\n        case 'huggingface':\n          response = await this.generateHuggingFaceResponse(userMessage, personality);\n          break;\n        default:\n          response = this.getFallbackResponse(personality, userMessage);\n      }\n    } catch (error) {\n      console.error('AI generation error:', error);\n      response = this.getFallbackResponse(personality, userMessage);\n    }\n\n    // Add AI response to history\n    this.addToHistory('assistant', response);\n    return response;\n  }\n\n  // Check if Ollama is available\n  async checkOllamaStatus() {\n    try {\n      const response = await fetch(`${this.baseUrl}/api/tags`);\n      return response.ok;\n    } catch (error) {\n      return false;\n    }\n  }\n\n  // Check if Hugging Face is available\n  async checkHuggingFaceStatus() {\n    try {\n      console.log('All environment variables:', process.env);\n      console.log('Looking for REACT_APP_HUGGINGFACE_TOKEN...');\n      const token = process.env.REACT_APP_HUGGINGFACE_TOKEN;\n      console.log('Token found:', token ? 'YES' : 'NO');\n      if (!token) {\n        console.error('Hugging Face token not found');\n        console.error('Available REACT_APP_ variables:', Object.keys(process.env).filter(key => key.startsWith('REACT_APP_')));\n        return false;\n      }\n      console.log('Testing Hugging Face connection with token:', token.substring(0, 10) + '...');\n\n      // Test with a simple model\n      const response = await fetch('https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium', {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${token}`\n        },\n        body: JSON.stringify({\n          inputs: 'Hello',\n          parameters: {\n            max_new_tokens: 10\n          }\n        })\n      });\n      console.log('Hugging Face response status:', response.status);\n      if (response.ok) {\n        console.log('Hugging Face connection successful');\n        return true;\n      } else if (response.status === 503) {\n        console.log('Hugging Face model is loading (503) - this is normal');\n        return true; // 503 means model is loading, which is fine\n      } else {\n        console.error('Hugging Face error:', response.status, response.statusText);\n        return false;\n      }\n    } catch (error) {\n      console.error('Hugging Face status check error:', error);\n      return false;\n    }\n  }\n\n  // Check AI status based on provider\n  async checkAIStatus() {\n    switch (this.provider) {\n      case 'ollama':\n        return await this.checkOllamaStatus();\n      case 'huggingface':\n        return await this.checkHuggingFaceStatus();\n      default:\n        return true;\n      // Fallback always works\n    }\n  }\n\n  // Get available models from Ollama\n  async getAvailableModels() {\n    try {\n      const response = await fetch(`${this.baseUrl}/api/tags`);\n      if (response.ok) {\n        const data = await response.json();\n        return data.models || [];\n      }\n      return [];\n    } catch (error) {\n      return [];\n    }\n  }\n}","map":{"version":3,"names":["AIService","constructor","provider","model","baseUrl","conversationHistory","maxHistory","setProvider","addToHistory","role","content","push","length","slice","generateOllamaResponse","userMessage","personality","prompt","buildPrompt","response","fetch","method","headers","body","JSON","stringify","stream","options","temperature","top_p","max_tokens","ok","Error","status","data","json","error","console","getFallbackResponse","generateHuggingFaceResponse","_data$","process","env","REACT_APP_HUGGINGFACE_TOKEN","inputs","parameters","max_new_tokens","generated_text","topWords","slangWords","formalityScore","messageCount","personalityContext","map","w","word","join","Array","from","history","msg","PersonalityEngine","require","engine","generateResponse","checkOllamaStatus","checkHuggingFaceStatus","log","token","Object","keys","filter","key","startsWith","substring","statusText","checkAIStatus","getAvailableModels","models"],"sources":["E:/Websites/EchoMe/src/utils/aiService.js"],"sourcesContent":["// AI Service for integrating with free AI models\r\n// Supports Ollama, Hugging Face, and other free AI providers\r\n\r\nexport class AIService {\r\n  constructor() {\r\n    this.provider = 'ollama'; // 'ollama', 'huggingface', 'local'\r\n    this.model = 'llama2'; // 'llama2', 'mistral', 'codellama'\r\n    this.baseUrl = 'http://localhost:11434'; // Ollama default URL\r\n    this.conversationHistory = [];\r\n    this.maxHistory = 10; // Keep last 10 messages for context\r\n  }\r\n\r\n  // Set AI provider and model\r\n  setProvider(provider, model = null) {\r\n    this.provider = provider;\r\n    if (model) this.model = model;\r\n    \r\n    // Set appropriate base URL\r\n    switch (provider) {\r\n      case 'ollama':\r\n        this.baseUrl = 'http://localhost:11434';\r\n        break;\r\n      case 'huggingface':\r\n        this.baseUrl = 'https://api-inference.huggingface.co/models';\r\n        break;\r\n      default:\r\n        this.baseUrl = 'http://localhost:11434';\r\n    }\r\n  }\r\n\r\n  // Add message to conversation history\r\n  addToHistory(role, content) {\r\n    this.conversationHistory.push({ role, content });\r\n    \r\n    // Keep only recent messages\r\n    if (this.conversationHistory.length > this.maxHistory * 2) {\r\n      this.conversationHistory = this.conversationHistory.slice(-this.maxHistory * 2);\r\n    }\r\n  }\r\n\r\n  // Generate AI response using Ollama\r\n  async generateOllamaResponse(userMessage, personality) {\r\n    try {\r\n      const prompt = this.buildPrompt(userMessage, personality);\r\n      \r\n      const response = await fetch(`${this.baseUrl}/api/generate`, {\r\n        method: 'POST',\r\n        headers: {\r\n          'Content-Type': 'application/json',\r\n        },\r\n        body: JSON.stringify({\r\n          model: this.model,\r\n          prompt: prompt,\r\n          stream: false,\r\n          options: {\r\n            temperature: 0.7,\r\n            top_p: 0.9,\r\n            max_tokens: 200\r\n          }\r\n        })\r\n      });\r\n\r\n      if (!response.ok) {\r\n        throw new Error(`Ollama API error: ${response.status}`);\r\n      }\r\n\r\n      const data = await response.json();\r\n      return data.response || 'Sorry, I couldn\\'t generate a response.';\r\n    } catch (error) {\r\n      console.error('Ollama API error:', error);\r\n      return this.getFallbackResponse(personality, userMessage);\r\n    }\r\n  }\r\n\r\n  // Generate AI response using Hugging Face\r\n  async generateHuggingFaceResponse(userMessage, personality) {\r\n    try {\r\n      const prompt = this.buildPrompt(userMessage, personality);\r\n      \r\n      const response = await fetch(`${this.baseUrl}/${this.model}`, {\r\n        method: 'POST',\r\n        headers: {\r\n          'Content-Type': 'application/json',\r\n          'Authorization': `Bearer ${process.env.REACT_APP_HUGGINGFACE_TOKEN || ''}`\r\n        },\r\n        body: JSON.stringify({\r\n          inputs: prompt,\r\n          parameters: {\r\n            max_new_tokens: 200,\r\n            temperature: 0.7,\r\n            top_p: 0.9\r\n          }\r\n        })\r\n      });\r\n\r\n      if (!response.ok) {\r\n        throw new Error(`Hugging Face API error: ${response.status}`);\r\n      }\r\n\r\n      const data = await response.json();\r\n      return data[0]?.generated_text || 'Sorry, I couldn\\'t generate a response.';\r\n    } catch (error) {\r\n      console.error('Hugging Face API error:', error);\r\n      return this.getFallbackResponse(personality, userMessage);\r\n    }\r\n  }\r\n\r\n  // Build context-aware prompt\r\n  buildPrompt(userMessage, personality) {\r\n    const { topWords, slangWords, formalityScore, messageCount } = personality;\r\n    \r\n    // Create personality context\r\n    let personalityContext = '';\r\n    if (messageCount > 5) {\r\n      personalityContext = `You are an AI that has learned to speak like the user. `;\r\n      \r\n      if (topWords.length > 0) {\r\n        personalityContext += `The user frequently uses words like: ${topWords.slice(0, 3).map(w => w.word).join(', ')}. `;\r\n      }\r\n      \r\n      if (slangWords.length > 0) {\r\n        personalityContext += `The user uses slang like: ${Array.from(slangWords).slice(0, 3).join(', ')}. `;\r\n      }\r\n      \r\n      if (formalityScore < 40) {\r\n        personalityContext += `The user speaks very casually and informally. `;\r\n      } else if (formalityScore > 70) {\r\n        personalityContext += `The user speaks formally and professionally. `;\r\n      }\r\n      \r\n      personalityContext += `Mirror their speaking style, vocabulary, and tone. `;\r\n    }\r\n\r\n    // Build conversation history\r\n    const history = this.conversationHistory\r\n      .map(msg => `${msg.role === 'user' ? 'User' : 'Assistant'}: ${msg.content}`)\r\n      .join('\\n');\r\n\r\n    // Create the full prompt\r\n    const prompt = `${personalityContext}\r\n\r\nConversation History:\r\n${history}\r\n\r\nUser: ${userMessage}\r\nAssistant:`;\r\n\r\n    return prompt;\r\n  }\r\n\r\n  // Fallback response when AI is unavailable\r\n  getFallbackResponse(personality, userMessage) {\r\n    // Use the existing personality engine as fallback\r\n    const { PersonalityEngine } = require('./personalityEngine');\r\n    const engine = new PersonalityEngine();\r\n    return engine.generateResponse(personality, userMessage);\r\n  }\r\n\r\n  // Main method to generate AI response\r\n  async generateResponse(userMessage, personality) {\r\n    // Add user message to history\r\n    this.addToHistory('user', userMessage);\r\n\r\n    let response = '';\r\n    \r\n    try {\r\n      switch (this.provider) {\r\n        case 'ollama':\r\n          response = await this.generateOllamaResponse(userMessage, personality);\r\n          break;\r\n        case 'huggingface':\r\n          response = await this.generateHuggingFaceResponse(userMessage, personality);\r\n          break;\r\n        default:\r\n          response = this.getFallbackResponse(personality, userMessage);\r\n      }\r\n    } catch (error) {\r\n      console.error('AI generation error:', error);\r\n      response = this.getFallbackResponse(personality, userMessage);\r\n    }\r\n\r\n    // Add AI response to history\r\n    this.addToHistory('assistant', response);\r\n    \r\n    return response;\r\n  }\r\n\r\n  // Check if Ollama is available\r\n  async checkOllamaStatus() {\r\n    try {\r\n      const response = await fetch(`${this.baseUrl}/api/tags`);\r\n      return response.ok;\r\n    } catch (error) {\r\n      return false;\r\n    }\r\n  }\r\n\r\n  // Check if Hugging Face is available\r\n  async checkHuggingFaceStatus() {\r\n    try {\r\n      console.log('All environment variables:', process.env);\r\n      console.log('Looking for REACT_APP_HUGGINGFACE_TOKEN...');\r\n      \r\n      const token = process.env.REACT_APP_HUGGINGFACE_TOKEN;\r\n      console.log('Token found:', token ? 'YES' : 'NO');\r\n      \r\n      if (!token) {\r\n        console.error('Hugging Face token not found');\r\n        console.error('Available REACT_APP_ variables:', Object.keys(process.env).filter(key => key.startsWith('REACT_APP_')));\r\n        return false;\r\n      }\r\n\r\n      console.log('Testing Hugging Face connection with token:', token.substring(0, 10) + '...');\r\n\r\n      // Test with a simple model\r\n      const response = await fetch('https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium', {\r\n        method: 'POST',\r\n        headers: {\r\n          'Content-Type': 'application/json',\r\n          'Authorization': `Bearer ${token}`\r\n        },\r\n        body: JSON.stringify({\r\n          inputs: 'Hello',\r\n          parameters: {\r\n            max_new_tokens: 10\r\n          }\r\n        })\r\n      });\r\n\r\n      console.log('Hugging Face response status:', response.status);\r\n      \r\n      if (response.ok) {\r\n        console.log('Hugging Face connection successful');\r\n        return true;\r\n      } else if (response.status === 503) {\r\n        console.log('Hugging Face model is loading (503) - this is normal');\r\n        return true; // 503 means model is loading, which is fine\r\n      } else {\r\n        console.error('Hugging Face error:', response.status, response.statusText);\r\n        return false;\r\n      }\r\n    } catch (error) {\r\n      console.error('Hugging Face status check error:', error);\r\n      return false;\r\n    }\r\n  }\r\n\r\n  // Check AI status based on provider\r\n  async checkAIStatus() {\r\n    switch (this.provider) {\r\n      case 'ollama':\r\n        return await this.checkOllamaStatus();\r\n      case 'huggingface':\r\n        return await this.checkHuggingFaceStatus();\r\n      default:\r\n        return true; // Fallback always works\r\n    }\r\n  }\r\n\r\n  // Get available models from Ollama\r\n  async getAvailableModels() {\r\n    try {\r\n      const response = await fetch(`${this.baseUrl}/api/tags`);\r\n      if (response.ok) {\r\n        const data = await response.json();\r\n        return data.models || [];\r\n      }\r\n      return [];\r\n    } catch (error) {\r\n      return [];\r\n    }\r\n  }\r\n} "],"mappings":"AAAA;AACA;;AAEA,OAAO,MAAMA,SAAS,CAAC;EACrBC,WAAWA,CAAA,EAAG;IACZ,IAAI,CAACC,QAAQ,GAAG,QAAQ,CAAC,CAAC;IAC1B,IAAI,CAACC,KAAK,GAAG,QAAQ,CAAC,CAAC;IACvB,IAAI,CAACC,OAAO,GAAG,wBAAwB,CAAC,CAAC;IACzC,IAAI,CAACC,mBAAmB,GAAG,EAAE;IAC7B,IAAI,CAACC,UAAU,GAAG,EAAE,CAAC,CAAC;EACxB;;EAEA;EACAC,WAAWA,CAACL,QAAQ,EAAEC,KAAK,GAAG,IAAI,EAAE;IAClC,IAAI,CAACD,QAAQ,GAAGA,QAAQ;IACxB,IAAIC,KAAK,EAAE,IAAI,CAACA,KAAK,GAAGA,KAAK;;IAE7B;IACA,QAAQD,QAAQ;MACd,KAAK,QAAQ;QACX,IAAI,CAACE,OAAO,GAAG,wBAAwB;QACvC;MACF,KAAK,aAAa;QAChB,IAAI,CAACA,OAAO,GAAG,6CAA6C;QAC5D;MACF;QACE,IAAI,CAACA,OAAO,GAAG,wBAAwB;IAC3C;EACF;;EAEA;EACAI,YAAYA,CAACC,IAAI,EAAEC,OAAO,EAAE;IAC1B,IAAI,CAACL,mBAAmB,CAACM,IAAI,CAAC;MAAEF,IAAI;MAAEC;IAAQ,CAAC,CAAC;;IAEhD;IACA,IAAI,IAAI,CAACL,mBAAmB,CAACO,MAAM,GAAG,IAAI,CAACN,UAAU,GAAG,CAAC,EAAE;MACzD,IAAI,CAACD,mBAAmB,GAAG,IAAI,CAACA,mBAAmB,CAACQ,KAAK,CAAC,CAAC,IAAI,CAACP,UAAU,GAAG,CAAC,CAAC;IACjF;EACF;;EAEA;EACA,MAAMQ,sBAAsBA,CAACC,WAAW,EAAEC,WAAW,EAAE;IACrD,IAAI;MACF,MAAMC,MAAM,GAAG,IAAI,CAACC,WAAW,CAACH,WAAW,EAAEC,WAAW,CAAC;MAEzD,MAAMG,QAAQ,GAAG,MAAMC,KAAK,CAAC,GAAG,IAAI,CAAChB,OAAO,eAAe,EAAE;QAC3DiB,MAAM,EAAE,MAAM;QACdC,OAAO,EAAE;UACP,cAAc,EAAE;QAClB,CAAC;QACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;UACnBtB,KAAK,EAAE,IAAI,CAACA,KAAK;UACjBc,MAAM,EAAEA,MAAM;UACdS,MAAM,EAAE,KAAK;UACbC,OAAO,EAAE;YACPC,WAAW,EAAE,GAAG;YAChBC,KAAK,EAAE,GAAG;YACVC,UAAU,EAAE;UACd;QACF,CAAC;MACH,CAAC,CAAC;MAEF,IAAI,CAACX,QAAQ,CAACY,EAAE,EAAE;QAChB,MAAM,IAAIC,KAAK,CAAC,qBAAqBb,QAAQ,CAACc,MAAM,EAAE,CAAC;MACzD;MAEA,MAAMC,IAAI,GAAG,MAAMf,QAAQ,CAACgB,IAAI,CAAC,CAAC;MAClC,OAAOD,IAAI,CAACf,QAAQ,IAAI,yCAAyC;IACnE,CAAC,CAAC,OAAOiB,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,mBAAmB,EAAEA,KAAK,CAAC;MACzC,OAAO,IAAI,CAACE,mBAAmB,CAACtB,WAAW,EAAED,WAAW,CAAC;IAC3D;EACF;;EAEA;EACA,MAAMwB,2BAA2BA,CAACxB,WAAW,EAAEC,WAAW,EAAE;IAC1D,IAAI;MAAA,IAAAwB,MAAA;MACF,MAAMvB,MAAM,GAAG,IAAI,CAACC,WAAW,CAACH,WAAW,EAAEC,WAAW,CAAC;MAEzD,MAAMG,QAAQ,GAAG,MAAMC,KAAK,CAAC,GAAG,IAAI,CAAChB,OAAO,IAAI,IAAI,CAACD,KAAK,EAAE,EAAE;QAC5DkB,MAAM,EAAE,MAAM;QACdC,OAAO,EAAE;UACP,cAAc,EAAE,kBAAkB;UAClC,eAAe,EAAE,UAAUmB,OAAO,CAACC,GAAG,CAACC,2BAA2B,IAAI,EAAE;QAC1E,CAAC;QACDpB,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;UACnBmB,MAAM,EAAE3B,MAAM;UACd4B,UAAU,EAAE;YACVC,cAAc,EAAE,GAAG;YACnBlB,WAAW,EAAE,GAAG;YAChBC,KAAK,EAAE;UACT;QACF,CAAC;MACH,CAAC,CAAC;MAEF,IAAI,CAACV,QAAQ,CAACY,EAAE,EAAE;QAChB,MAAM,IAAIC,KAAK,CAAC,2BAA2Bb,QAAQ,CAACc,MAAM,EAAE,CAAC;MAC/D;MAEA,MAAMC,IAAI,GAAG,MAAMf,QAAQ,CAACgB,IAAI,CAAC,CAAC;MAClC,OAAO,EAAAK,MAAA,GAAAN,IAAI,CAAC,CAAC,CAAC,cAAAM,MAAA,uBAAPA,MAAA,CAASO,cAAc,KAAI,yCAAyC;IAC7E,CAAC,CAAC,OAAOX,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,yBAAyB,EAAEA,KAAK,CAAC;MAC/C,OAAO,IAAI,CAACE,mBAAmB,CAACtB,WAAW,EAAED,WAAW,CAAC;IAC3D;EACF;;EAEA;EACAG,WAAWA,CAACH,WAAW,EAAEC,WAAW,EAAE;IACpC,MAAM;MAAEgC,QAAQ;MAAEC,UAAU;MAAEC,cAAc;MAAEC;IAAa,CAAC,GAAGnC,WAAW;;IAE1E;IACA,IAAIoC,kBAAkB,GAAG,EAAE;IAC3B,IAAID,YAAY,GAAG,CAAC,EAAE;MACpBC,kBAAkB,GAAG,yDAAyD;MAE9E,IAAIJ,QAAQ,CAACpC,MAAM,GAAG,CAAC,EAAE;QACvBwC,kBAAkB,IAAI,wCAAwCJ,QAAQ,CAACnC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAACwC,GAAG,CAACC,CAAC,IAAIA,CAAC,CAACC,IAAI,CAAC,CAACC,IAAI,CAAC,IAAI,CAAC,IAAI;MACpH;MAEA,IAAIP,UAAU,CAACrC,MAAM,GAAG,CAAC,EAAE;QACzBwC,kBAAkB,IAAI,6BAA6BK,KAAK,CAACC,IAAI,CAACT,UAAU,CAAC,CAACpC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC2C,IAAI,CAAC,IAAI,CAAC,IAAI;MACtG;MAEA,IAAIN,cAAc,GAAG,EAAE,EAAE;QACvBE,kBAAkB,IAAI,gDAAgD;MACxE,CAAC,MAAM,IAAIF,cAAc,GAAG,EAAE,EAAE;QAC9BE,kBAAkB,IAAI,+CAA+C;MACvE;MAEAA,kBAAkB,IAAI,qDAAqD;IAC7E;;IAEA;IACA,MAAMO,OAAO,GAAG,IAAI,CAACtD,mBAAmB,CACrCgD,GAAG,CAACO,GAAG,IAAI,GAAGA,GAAG,CAACnD,IAAI,KAAK,MAAM,GAAG,MAAM,GAAG,WAAW,KAAKmD,GAAG,CAAClD,OAAO,EAAE,CAAC,CAC3E8C,IAAI,CAAC,IAAI,CAAC;;IAEb;IACA,MAAMvC,MAAM,GAAG,GAAGmC,kBAAkB;AACxC;AACA;AACA,EAAEO,OAAO;AACT;AACA,QAAQ5C,WAAW;AACnB,WAAW;IAEP,OAAOE,MAAM;EACf;;EAEA;EACAqB,mBAAmBA,CAACtB,WAAW,EAAED,WAAW,EAAE;IAC5C;IACA,MAAM;MAAE8C;IAAkB,CAAC,GAAGC,OAAO,CAAC,qBAAqB,CAAC;IAC5D,MAAMC,MAAM,GAAG,IAAIF,iBAAiB,CAAC,CAAC;IACtC,OAAOE,MAAM,CAACC,gBAAgB,CAAChD,WAAW,EAAED,WAAW,CAAC;EAC1D;;EAEA;EACA,MAAMiD,gBAAgBA,CAACjD,WAAW,EAAEC,WAAW,EAAE;IAC/C;IACA,IAAI,CAACR,YAAY,CAAC,MAAM,EAAEO,WAAW,CAAC;IAEtC,IAAII,QAAQ,GAAG,EAAE;IAEjB,IAAI;MACF,QAAQ,IAAI,CAACjB,QAAQ;QACnB,KAAK,QAAQ;UACXiB,QAAQ,GAAG,MAAM,IAAI,CAACL,sBAAsB,CAACC,WAAW,EAAEC,WAAW,CAAC;UACtE;QACF,KAAK,aAAa;UAChBG,QAAQ,GAAG,MAAM,IAAI,CAACoB,2BAA2B,CAACxB,WAAW,EAAEC,WAAW,CAAC;UAC3E;QACF;UACEG,QAAQ,GAAG,IAAI,CAACmB,mBAAmB,CAACtB,WAAW,EAAED,WAAW,CAAC;MACjE;IACF,CAAC,CAAC,OAAOqB,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,sBAAsB,EAAEA,KAAK,CAAC;MAC5CjB,QAAQ,GAAG,IAAI,CAACmB,mBAAmB,CAACtB,WAAW,EAAED,WAAW,CAAC;IAC/D;;IAEA;IACA,IAAI,CAACP,YAAY,CAAC,WAAW,EAAEW,QAAQ,CAAC;IAExC,OAAOA,QAAQ;EACjB;;EAEA;EACA,MAAM8C,iBAAiBA,CAAA,EAAG;IACxB,IAAI;MACF,MAAM9C,QAAQ,GAAG,MAAMC,KAAK,CAAC,GAAG,IAAI,CAAChB,OAAO,WAAW,CAAC;MACxD,OAAOe,QAAQ,CAACY,EAAE;IACpB,CAAC,CAAC,OAAOK,KAAK,EAAE;MACd,OAAO,KAAK;IACd;EACF;;EAEA;EACA,MAAM8B,sBAAsBA,CAAA,EAAG;IAC7B,IAAI;MACF7B,OAAO,CAAC8B,GAAG,CAAC,4BAA4B,EAAE1B,OAAO,CAACC,GAAG,CAAC;MACtDL,OAAO,CAAC8B,GAAG,CAAC,4CAA4C,CAAC;MAEzD,MAAMC,KAAK,GAAG3B,OAAO,CAACC,GAAG,CAACC,2BAA2B;MACrDN,OAAO,CAAC8B,GAAG,CAAC,cAAc,EAAEC,KAAK,GAAG,KAAK,GAAG,IAAI,CAAC;MAEjD,IAAI,CAACA,KAAK,EAAE;QACV/B,OAAO,CAACD,KAAK,CAAC,8BAA8B,CAAC;QAC7CC,OAAO,CAACD,KAAK,CAAC,iCAAiC,EAAEiC,MAAM,CAACC,IAAI,CAAC7B,OAAO,CAACC,GAAG,CAAC,CAAC6B,MAAM,CAACC,GAAG,IAAIA,GAAG,CAACC,UAAU,CAAC,YAAY,CAAC,CAAC,CAAC;QACtH,OAAO,KAAK;MACd;MAEApC,OAAO,CAAC8B,GAAG,CAAC,6CAA6C,EAAEC,KAAK,CAACM,SAAS,CAAC,CAAC,EAAE,EAAE,CAAC,GAAG,KAAK,CAAC;;MAE1F;MACA,MAAMvD,QAAQ,GAAG,MAAMC,KAAK,CAAC,uEAAuE,EAAE;QACpGC,MAAM,EAAE,MAAM;QACdC,OAAO,EAAE;UACP,cAAc,EAAE,kBAAkB;UAClC,eAAe,EAAE,UAAU8C,KAAK;QAClC,CAAC;QACD7C,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;UACnBmB,MAAM,EAAE,OAAO;UACfC,UAAU,EAAE;YACVC,cAAc,EAAE;UAClB;QACF,CAAC;MACH,CAAC,CAAC;MAEFT,OAAO,CAAC8B,GAAG,CAAC,+BAA+B,EAAEhD,QAAQ,CAACc,MAAM,CAAC;MAE7D,IAAId,QAAQ,CAACY,EAAE,EAAE;QACfM,OAAO,CAAC8B,GAAG,CAAC,oCAAoC,CAAC;QACjD,OAAO,IAAI;MACb,CAAC,MAAM,IAAIhD,QAAQ,CAACc,MAAM,KAAK,GAAG,EAAE;QAClCI,OAAO,CAAC8B,GAAG,CAAC,sDAAsD,CAAC;QACnE,OAAO,IAAI,CAAC,CAAC;MACf,CAAC,MAAM;QACL9B,OAAO,CAACD,KAAK,CAAC,qBAAqB,EAAEjB,QAAQ,CAACc,MAAM,EAAEd,QAAQ,CAACwD,UAAU,CAAC;QAC1E,OAAO,KAAK;MACd;IACF,CAAC,CAAC,OAAOvC,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,kCAAkC,EAAEA,KAAK,CAAC;MACxD,OAAO,KAAK;IACd;EACF;;EAEA;EACA,MAAMwC,aAAaA,CAAA,EAAG;IACpB,QAAQ,IAAI,CAAC1E,QAAQ;MACnB,KAAK,QAAQ;QACX,OAAO,MAAM,IAAI,CAAC+D,iBAAiB,CAAC,CAAC;MACvC,KAAK,aAAa;QAChB,OAAO,MAAM,IAAI,CAACC,sBAAsB,CAAC,CAAC;MAC5C;QACE,OAAO,IAAI;MAAE;IACjB;EACF;;EAEA;EACA,MAAMW,kBAAkBA,CAAA,EAAG;IACzB,IAAI;MACF,MAAM1D,QAAQ,GAAG,MAAMC,KAAK,CAAC,GAAG,IAAI,CAAChB,OAAO,WAAW,CAAC;MACxD,IAAIe,QAAQ,CAACY,EAAE,EAAE;QACf,MAAMG,IAAI,GAAG,MAAMf,QAAQ,CAACgB,IAAI,CAAC,CAAC;QAClC,OAAOD,IAAI,CAAC4C,MAAM,IAAI,EAAE;MAC1B;MACA,OAAO,EAAE;IACX,CAAC,CAAC,OAAO1C,KAAK,EAAE;MACd,OAAO,EAAE;IACX;EACF;AACF","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}